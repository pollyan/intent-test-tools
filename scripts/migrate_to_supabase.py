#!/usr/bin/env python3
"""
SQLiteåˆ°Supabase PostgreSQLæ•°æ®è¿ç§»è„šæœ¬

ä½¿ç”¨æ–¹æ³•:
    python scripts/migrate_to_supabase.py --source sqlite:///path/to/db.db --target postgresql://...
    python scripts/migrate_to_supabase.py --auto  # è‡ªåŠ¨æ£€æµ‹é…ç½®
"""

import os
import sys
import argparse
import json
from datetime import datetime
from typing import Dict, List, Any
import sqlite3
import psycopg2
from sqlalchemy import create_engine, text, MetaData, Table
from sqlalchemy.orm import sessionmaker

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from web_gui.database_config import DatabaseConfig


class DatabaseMigrator:
    """æ•°æ®åº“è¿ç§»å™¨"""
    
    def __init__(self, source_url: str, target_url: str):
        self.source_url = source_url
        self.target_url = target_url
        self.source_engine = create_engine(source_url)
        self.target_engine = create_engine(target_url)
        self.migration_log = []
    
    def log(self, message: str, level: str = "INFO"):
        """è®°å½•è¿ç§»æ—¥å¿—"""
        timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        log_entry = f"[{timestamp}] {level}: {message}"
        print(log_entry)
        self.migration_log.append(log_entry)
    
    def validate_connections(self) -> bool:
        """éªŒè¯æ•°æ®åº“è¿æ¥"""
        try:
            # æµ‹è¯•æºæ•°æ®åº“è¿æ¥
            with self.source_engine.connect() as conn:
                conn.execute(text("SELECT 1"))
            self.log("âœ… æºæ•°æ®åº“è¿æ¥æˆåŠŸ")
            
            # æµ‹è¯•ç›®æ ‡æ•°æ®åº“è¿æ¥
            with self.target_engine.connect() as conn:
                conn.execute(text("SELECT 1"))
            self.log("âœ… ç›®æ ‡æ•°æ®åº“è¿æ¥æˆåŠŸ")
            
            return True
        except Exception as e:
            self.log(f"âŒ æ•°æ®åº“è¿æ¥å¤±è´¥: {e}", "ERROR")
            return False
    
    def get_table_schema(self, engine) -> Dict[str, Any]:
        """è·å–è¡¨ç»“æ„"""
        metadata = MetaData()
        metadata.reflect(bind=engine)
        
        schema = {}
        for table_name, table in metadata.tables.items():
            schema[table_name] = {
                'columns': [
                    {
                        'name': col.name,
                        'type': str(col.type),
                        'nullable': col.nullable,
                        'primary_key': col.primary_key,
                        'foreign_keys': [str(fk) for fk in col.foreign_keys]
                    }
                    for col in table.columns
                ],
                'indexes': [str(idx) for idx in table.indexes],
                'foreign_keys': [str(fk) for fk in table.foreign_key_constraints]
            }
        
        return schema
    
    def create_postgresql_schema(self) -> bool:
        """åœ¨PostgreSQLä¸­åˆ›å»ºè¡¨ç»“æ„"""
        try:
            # PostgreSQLå»ºè¡¨è¯­å¥
            create_tables_sql = """
            -- æµ‹è¯•ç”¨ä¾‹è¡¨
            CREATE TABLE IF NOT EXISTS test_cases (
                id SERIAL PRIMARY KEY,
                name VARCHAR(255) NOT NULL,
                description TEXT,
                steps TEXT NOT NULL,
                tags VARCHAR(500),
                category VARCHAR(100),
                priority INTEGER DEFAULT 3,
                created_by VARCHAR(100),
                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                is_active BOOLEAN DEFAULT TRUE
            );
            
            -- æ‰§è¡Œå†å²è¡¨
            CREATE TABLE IF NOT EXISTS execution_history (
                id SERIAL PRIMARY KEY,
                execution_id VARCHAR(50) UNIQUE NOT NULL,
                test_case_id INTEGER NOT NULL,
                status VARCHAR(50) NOT NULL,
                mode VARCHAR(20) DEFAULT 'normal',
                browser VARCHAR(50) DEFAULT 'chrome',
                start_time TIMESTAMP NOT NULL,
                end_time TIMESTAMP,
                duration INTEGER,
                steps_total INTEGER,
                steps_passed INTEGER,
                steps_failed INTEGER,
                result_summary TEXT,
                screenshots_path TEXT,
                logs_path TEXT,
                error_message TEXT,
                error_stack TEXT,
                executed_by VARCHAR(100),
                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                FOREIGN KEY (test_case_id) REFERENCES test_cases(id)
            );
            
            -- æ­¥éª¤æ‰§è¡Œè¯¦æƒ…è¡¨
            CREATE TABLE IF NOT EXISTS step_executions (
                id SERIAL PRIMARY KEY,
                execution_id VARCHAR(50) NOT NULL,
                step_index INTEGER NOT NULL,
                step_description TEXT NOT NULL,
                status VARCHAR(20) NOT NULL,
                start_time TIMESTAMP NOT NULL,
                end_time TIMESTAMP,
                duration INTEGER,
                screenshot_path TEXT,
                ai_confidence REAL,
                ai_decision TEXT,
                error_message TEXT,
                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                FOREIGN KEY (execution_id) REFERENCES execution_history(execution_id)
            );
            
            -- æ¨¡æ¿è¡¨
            CREATE TABLE IF NOT EXISTS templates (
                id SERIAL PRIMARY KEY,
                name VARCHAR(255) NOT NULL,
                description TEXT,
                category VARCHAR(100),
                steps_template TEXT NOT NULL,
                parameters TEXT,
                usage_count INTEGER DEFAULT 0,
                created_by VARCHAR(100),
                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                is_public BOOLEAN DEFAULT FALSE
            );
            
            -- åˆ›å»ºç´¢å¼•
            CREATE INDEX IF NOT EXISTS idx_test_cases_name ON test_cases(name);
            CREATE INDEX IF NOT EXISTS idx_test_cases_category ON test_cases(category);
            CREATE INDEX IF NOT EXISTS idx_test_cases_created_at ON test_cases(created_at);
            CREATE INDEX IF NOT EXISTS idx_execution_history_test_case_id ON execution_history(test_case_id);
            CREATE INDEX IF NOT EXISTS idx_execution_history_status ON execution_history(status);
            CREATE INDEX IF NOT EXISTS idx_execution_history_start_time ON execution_history(start_time);
            CREATE INDEX IF NOT EXISTS idx_step_executions_execution_id ON step_executions(execution_id);
            CREATE INDEX IF NOT EXISTS idx_templates_category ON templates(category);
            """
            
            with self.target_engine.connect() as conn:
                # åˆ†å‰²å¹¶æ‰§è¡Œæ¯ä¸ªè¯­å¥
                statements = [stmt.strip() for stmt in create_tables_sql.split(';') if stmt.strip()]
                for stmt in statements:
                    conn.execute(text(stmt))
                conn.commit()
            
            self.log("âœ… PostgreSQLè¡¨ç»“æ„åˆ›å»ºæˆåŠŸ")
            return True
            
        except Exception as e:
            self.log(f"âŒ åˆ›å»ºPostgreSQLè¡¨ç»“æ„å¤±è´¥: {e}", "ERROR")
            return False
    
    def migrate_table_data(self, table_name: str) -> bool:
        """è¿ç§»å•ä¸ªè¡¨çš„æ•°æ®"""
        try:
            self.log(f"ğŸ”„ å¼€å§‹è¿ç§»è¡¨: {table_name}")
            
            # ä»æºæ•°æ®åº“è¯»å–æ•°æ®
            with self.source_engine.connect() as source_conn:
                result = source_conn.execute(text(f"SELECT * FROM {table_name}"))
                rows = result.fetchall()
                columns = result.keys()
            
            if not rows:
                self.log(f"âš ï¸  è¡¨ {table_name} æ— æ•°æ®ï¼Œè·³è¿‡")
                return True
            
            self.log(f"ğŸ“Š è¡¨ {table_name} å…±æœ‰ {len(rows)} æ¡è®°å½•")
            
            # æ„å»ºæ’å…¥è¯­å¥
            column_names = ', '.join(columns)
            placeholders = ', '.join([f'%({col})s' for col in columns])
            insert_sql = f"INSERT INTO {table_name} ({column_names}) VALUES ({placeholders})"
            
            # è½¬æ¢æ•°æ®æ ¼å¼
            data_to_insert = []
            for row in rows:
                row_dict = dict(zip(columns, row))
                # å¤„ç†ç‰¹æ®Šå­—æ®µç±»å‹è½¬æ¢
                if table_name == 'test_cases' and 'id' in row_dict:
                    # PostgreSQLä½¿ç”¨SERIALï¼Œä¸éœ€è¦æ’å…¥id
                    row_dict.pop('id', None)
                data_to_insert.append(row_dict)
            
            # æ‰¹é‡æ’å…¥åˆ°ç›®æ ‡æ•°æ®åº“
            with self.target_engine.connect() as target_conn:
                if table_name == 'test_cases':
                    # å¯¹äºtest_casesè¡¨ï¼Œéœ€è¦ç‰¹æ®Šå¤„ç†idå­—æ®µ
                    insert_sql = insert_sql.replace('id, ', '').replace('%(id)s, ', '')
                
                for row_data in data_to_insert:
                    target_conn.execute(text(insert_sql), row_data)
                target_conn.commit()
            
            self.log(f"âœ… è¡¨ {table_name} è¿ç§»å®Œæˆï¼Œå…± {len(data_to_insert)} æ¡è®°å½•")
            return True
            
        except Exception as e:
            self.log(f"âŒ è¿ç§»è¡¨ {table_name} å¤±è´¥: {e}", "ERROR")
            return False
    
    def migrate_all_data(self) -> bool:
        """è¿ç§»æ‰€æœ‰æ•°æ®"""
        # æŒ‰ä¾èµ–é¡ºåºè¿ç§»è¡¨
        tables_order = [
            'test_cases',
            'templates', 
            'execution_history',
            'step_executions'
        ]
        
        success_count = 0
        for table_name in tables_order:
            if self.migrate_table_data(table_name):
                success_count += 1
            else:
                self.log(f"âŒ åœæ­¢è¿ç§»ï¼Œè¡¨ {table_name} è¿ç§»å¤±è´¥", "ERROR")
                return False
        
        self.log(f"ğŸ‰ æ•°æ®è¿ç§»å®Œæˆï¼æˆåŠŸè¿ç§» {success_count} ä¸ªè¡¨")
        return True
    
    def verify_migration(self) -> bool:
        """éªŒè¯è¿ç§»ç»“æœ"""
        try:
            self.log("ğŸ” å¼€å§‹éªŒè¯è¿ç§»ç»“æœ...")
            
            tables = ['test_cases', 'execution_history', 'step_executions', 'templates']
            
            for table_name in tables:
                # æ£€æŸ¥æºæ•°æ®åº“è®°å½•æ•°
                with self.source_engine.connect() as source_conn:
                    source_result = source_conn.execute(text(f"SELECT COUNT(*) FROM {table_name}"))
                    source_count = source_result.scalar()
                
                # æ£€æŸ¥ç›®æ ‡æ•°æ®åº“è®°å½•æ•°
                with self.target_engine.connect() as target_conn:
                    target_result = target_conn.execute(text(f"SELECT COUNT(*) FROM {table_name}"))
                    target_count = target_result.scalar()
                
                if source_count == target_count:
                    self.log(f"âœ… è¡¨ {table_name}: {source_count} -> {target_count} è®°å½•")
                else:
                    self.log(f"âš ï¸  è¡¨ {table_name}: {source_count} -> {target_count} è®°å½•ä¸åŒ¹é…", "WARNING")
            
            self.log("âœ… è¿ç§»éªŒè¯å®Œæˆ")
            return True
            
        except Exception as e:
            self.log(f"âŒ éªŒè¯è¿ç§»å¤±è´¥: {e}", "ERROR")
            return False
    
    def run_migration(self) -> bool:
        """æ‰§è¡Œå®Œæ•´è¿ç§»æµç¨‹"""
        self.log("ğŸš€ å¼€å§‹æ•°æ®åº“è¿ç§»...")
        
        # 1. éªŒè¯è¿æ¥
        if not self.validate_connections():
            return False
        
        # 2. åˆ›å»ºç›®æ ‡è¡¨ç»“æ„
        if not self.create_postgresql_schema():
            return False
        
        # 3. è¿ç§»æ•°æ®
        if not self.migrate_all_data():
            return False
        
        # 4. éªŒè¯è¿ç§»ç»“æœ
        if not self.verify_migration():
            return False
        
        self.log("ğŸ‰ æ•°æ®åº“è¿ç§»æˆåŠŸå®Œæˆï¼")
        return True
    
    def save_migration_log(self, log_file: str = None):
        """ä¿å­˜è¿ç§»æ—¥å¿—"""
        if not log_file:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            log_file = f"migration_log_{timestamp}.txt"
        
        with open(log_file, 'w', encoding='utf-8') as f:
            f.write('\n'.join(self.migration_log))
        
        print(f"ğŸ“ è¿ç§»æ—¥å¿—å·²ä¿å­˜åˆ°: {log_file}")


def main():
    parser = argparse.ArgumentParser(description="SQLiteåˆ°Supabase PostgreSQLæ•°æ®è¿ç§»å·¥å…·")
    parser.add_argument('--source', help='æºæ•°æ®åº“URL (SQLite)')
    parser.add_argument('--target', help='ç›®æ ‡æ•°æ®åº“URL (PostgreSQL)')
    parser.add_argument('--auto', action='store_true', help='è‡ªåŠ¨æ£€æµ‹é…ç½®')
    parser.add_argument('--log', help='æ—¥å¿—æ–‡ä»¶è·¯å¾„')
    
    args = parser.parse_args()
    
    if args.auto:
        # è‡ªåŠ¨æ£€æµ‹é…ç½®
        db_config = DatabaseConfig()
        
        # æºæ•°æ®åº“ (SQLite)
        instance_path = os.path.join(os.path.dirname(os.path.dirname(os.path.abspath(__file__))), 'web_gui', 'instance')
        source_db_path = os.path.join(instance_path, 'gui_test_cases.db')
        source_url = f'sqlite:///{source_db_path}'
        
        # ç›®æ ‡æ•°æ®åº“ (ä»ç¯å¢ƒå˜é‡è·å–)
        target_url = os.getenv('SUPABASE_DATABASE_URL') or os.getenv('DATABASE_URL')
        
        if not target_url:
            print("âŒ è¯·è®¾ç½® SUPABASE_DATABASE_URL æˆ– DATABASE_URL ç¯å¢ƒå˜é‡")
            return False
        
        print(f"ğŸ” è‡ªåŠ¨æ£€æµ‹é…ç½®:")
        print(f"   æºæ•°æ®åº“: {source_url}")
        print(f"   ç›®æ ‡æ•°æ®åº“: {target_url[:50]}...")
        
    else:
        if not args.source or not args.target:
            print("âŒ è¯·æä¾›æºæ•°æ®åº“å’Œç›®æ ‡æ•°æ®åº“URLï¼Œæˆ–ä½¿ç”¨ --auto å‚æ•°")
            return False
        
        source_url = args.source
        target_url = args.target
    
    # æ‰§è¡Œè¿ç§»
    migrator = DatabaseMigrator(source_url, target_url)
    success = migrator.run_migration()
    
    # ä¿å­˜æ—¥å¿—
    migrator.save_migration_log(args.log)
    
    return success


if __name__ == '__main__':
    success = main()
    sys.exit(0 if success else 1)
